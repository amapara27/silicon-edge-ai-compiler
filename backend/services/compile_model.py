from dataclasses import dataclass
from typing import Optional
import numpy as np
import onnx

# compiled model class
@dataclass
class CompiledModel:
    source_code: str
    header_code: str
    model_name: str


# ONNX data type to C type mapping
C_DTYPE_MAP = {
    "float32": "float",
    "float16": "float",
    "float64": "double",
    "int8": "int8_t",
    "uint8": "uint8_t",
    "int16": "int16_t",
    "uint16": "uint16_t",
    "int32": "int32_t",
    "uint32": "uint32_t",
    "int64": "int64_t",
    "uint64": "uint64_t",
}


def _format_weight_array(data: np.ndarray, name: str, dtype: str = "float") -> str:
    """Format numpy array as C array initializer"""
    flat = data.flatten()
    # Format values with appropriate precision
    if dtype in ["float", "double"]:
        values = ", ".join(f"{v:.8f}f" for v in flat)
    else:
        values = ", ".join(str(int(v)) for v in flat)
    
    shape_comment = f"// Shape: {list(data.shape)}, Size: {flat.size}"
    return f"{shape_comment}\nstatic const {dtype} {name}[{flat.size}] = {{\n    {values}\n}};"


def _get_weight_data(model: onnx.ModelProto, name: str) -> Optional[np.ndarray]:
    """Extract weight data from ONNX initializer"""
    for init in model.graph.initializer:
        if init.name == name:
            return onnx.numpy_helper.to_array(init)
    return None


# generates header file
def generate_header(model_name: str, model_info: dict, target_chip: str) -> str:
    inputs = model_info.get("inputs", [])
    outputs = model_info.get("outputs", [])
    
    # Calculate input/output sizes
    input_size = 1
    if inputs and inputs[0].get("shape"):
        for dim in inputs[0]["shape"]:
            if isinstance(dim, int) and dim > 0:
                input_size *= dim
    
    output_size = 1
    if outputs and outputs[0].get("shape"):
        for dim in outputs[0]["shape"]:
            if isinstance(dim, int) and dim > 0:
                output_size *= dim
    
    header = f"""/**
 * {model_name}.h - Generated Neural Network Header
 * Target: {target_chip}
 * 
 * Auto-generated by Silicon Edge AI Compiler
 */

#ifndef {model_name.upper()}_H
#define {model_name.upper()}_H

#include <stdint.h>
#include <stddef.h>

#ifdef __cplusplus
extern "C" {{
#endif

/* Model configuration */
#define {model_name.upper()}_INPUT_SIZE  {input_size}
#define {model_name.upper()}_OUTPUT_SIZE {output_size}
#define {model_name.upper()}_TOTAL_PARAMS {model_info.get('total_parameters', 0)}

/* Initialize the neural network */
void {model_name}_init(void);

/* Run forward inference */
void {model_name}_forward(const float* input, float* output);

/* Get model info */
size_t {model_name}_get_input_size(void);
size_t {model_name}_get_output_size(void);

#ifdef __cplusplus
}}
#endif

#endif /* {model_name.upper()}_H */
"""
    return header

# generates source file
def generate_source(model_name: str, model_info: dict, model: onnx.ModelProto, target_chip: str):
    # gets the model info from the generated dict
    layers = model_info.get("layers", [])
    weights_info = model_info.get("weights", [])
    inputs = model_info.get("inputs", [])
    outputs = model_info.get("outputs", [])
    
    # calculate sizes
    input_size = 1
    if inputs and inputs[0].get("shape"):
        for dim in inputs[0]["shape"]:
            if isinstance(dim, int) and dim > 0:
                input_size *= dim
    
    output_size = 1
    if outputs and outputs[0].get("shape"):
        for dim in outputs[0]["shape"]:
            if isinstance(dim, int) and dim > 0:
                output_size *= dim
    
    # generate weight arrays
    weight_sections = []
    for w in weights_info:
        data = _get_weight_data(model, w["name"])
        if data is not None:
            c_dtype = C_DTYPE_MAP.get(w["dtype"], "float")
            safe_name = w["name"].replace(".", "_").replace("/", "_")
            weight_sections.append(_format_weight_array(data, safe_name, c_dtype))
    
    weights_code = "\n\n".join(weight_sections) if weight_sections else "/* No weights */"
    
    # calculate buffer sizes for intermediate activations
    max_buffer_size = max(input_size, output_size, 1024)  # At least 1KB
    
    # generate layer forward pass code
    layer_code_lines = []
    prev_output = "input"
    
    for i, layer in enumerate(layers):
        op_type = layer.get("op_type", "Unknown")
        layer_inputs = layer.get("inputs", [])
        layer_outputs = layer.get("outputs", [])
        
        # get weight/bias names for this layer
        weight_name = None
        bias_name = None
        if len(layer_inputs) > 1:
            weight_name = layer_inputs[1].replace(".", "_").replace("/", "_")
        if len(layer_inputs) > 2:
            bias_name = layer_inputs[2].replace(".", "_").replace("/", "_")
        
        # find weight shape
        weight_shape = None
        for w in weights_info:
            if w["name"] == layer_inputs[1] if len(layer_inputs) > 1 else None:
                weight_shape = w["shape"]
                break
        
        curr_output = f"layer_{i}_out"
        
        if op_type in ["Gemm", "MatMul"]:
            # Dense/Fully connected layer
            out_features = weight_shape[0] if weight_shape else 128
            layer_code_lines.append(f"""
    /* Layer {i}: Dense ({op_type}) */
    static float {curr_output}[{out_features}];
    dense_forward({prev_output}, {weight_name}, {bias_name if bias_name else "NULL"}, {curr_output}, 
                  {input_size if i == 0 else "prev_size"}, {out_features});""")
            prev_output = curr_output
            
        elif op_type == "Relu":
            layer_code_lines.append(f"""
    /* Layer {i}: ReLU */
    relu_forward({prev_output}, {prev_output}, layer_size);""")
            
        elif op_type == "Sigmoid":
            layer_code_lines.append(f"""
    /* Layer {i}: Sigmoid */
    sigmoid_forward({prev_output}, {prev_output}, layer_size);""")
            
        elif op_type == "Tanh":
            layer_code_lines.append(f"""
    /* Layer {i}: Tanh */
    tanh_forward({prev_output}, {prev_output}, layer_size);""")
            
        elif op_type == "Softmax":
            layer_code_lines.append(f"""
    /* Layer {i}: Softmax */
    softmax_forward({prev_output}, output, {output_size});""")
            prev_output = "output"
            
        elif op_type in ["Add", "Flatten", "Reshape", "Dropout"]:
            # Pass-through or simple operations
            layer_code_lines.append(f"""
    /* Layer {i}: {op_type} (pass-through) */""")
    
    layers_code = "\n".join(layer_code_lines) if layer_code_lines else "    /* No layers */"
    
    source = f"""/**
 * {model_name}.c - Generated Neural Network Implementation
 * Target: {target_chip}
 * Total Parameters: {model_info.get('total_parameters', 0):,}
 * 
 * Auto-generated by Silicon Edge AI Compiler
 */

#include "{model_name}.h"
#include <math.h>
#include <string.h>

/* ============= Weights and Biases ============= */

{weights_code}

/* ============= Activation Functions ============= */

static void relu_forward(const float* in, float* out, size_t size) {{
    for (size_t i = 0; i < size; i++) {{
        out[i] = in[i] > 0.0f ? in[i] : 0.0f;
    }}
}}

static void sigmoid_forward(const float* in, float* out, size_t size) {{
    for (size_t i = 0; i < size; i++) {{
        out[i] = 1.0f / (1.0f + expf(-in[i]));
    }}
}}

static void tanh_forward(const float* in, float* out, size_t size) {{
    for (size_t i = 0; i < size; i++) {{
        out[i] = tanhf(in[i]);
    }}
}}

static void softmax_forward(const float* in, float* out, size_t size) {{
    float max_val = in[0];
    for (size_t i = 1; i < size; i++) {{
        if (in[i] > max_val) max_val = in[i];
    }}
    
    float sum = 0.0f;
    for (size_t i = 0; i < size; i++) {{
        out[i] = expf(in[i] - max_val);
        sum += out[i];
    }}
    
    for (size_t i = 0; i < size; i++) {{
        out[i] /= sum;
    }}
}}

/* ============= Layer Functions ============= */

static void dense_forward(
    const float* input, 
    const float* weights,
    const float* bias,
    float* output,
    size_t in_features,
    size_t out_features
) {{
    for (size_t o = 0; o < out_features; o++) {{
        float sum = bias ? bias[o] : 0.0f;
        for (size_t i = 0; i < in_features; i++) {{
            sum += input[i] * weights[o * in_features + i];
        }}
        output[o] = sum;
    }}
}}

/* ============= Model Functions ============= */

void {model_name}_init(void) {{
    /* 
     * Initialize model - called once at startup.
     * Add any hardware-specific initialization here.
     */
}}

void {model_name}_forward(const float* input, float* output) {{
    /* Forward pass through all layers */
    size_t layer_size = {input_size};
    (void)layer_size; /* Suppress unused warning if needed */
{layers_code}
    
    /* Copy final result to output if not already there */
    /* memcpy(output, final_layer, sizeof(float) * {output_size}); */
}}

size_t {model_name}_get_input_size(void) {{
    return {model_name.upper()}_INPUT_SIZE;
}}

size_t {model_name}_get_output_size(void) {{
    return {model_name.upper()}_OUTPUT_SIZE;
}}

/* ============= Python/C Inference Comparison Helper ============= */

#ifdef ENABLE_INFERENCE_TEST
/*
 * Test inference with known input.
 * Use this to compare Python vs C inference results.
 */
#include <stdio.h>

void {model_name}_test_inference(void) {{
    float test_input[{input_size}];
    float test_output[{output_size}];
    
    /* Fill with test values */
    for (size_t i = 0; i < {input_size}; i++) {{
        test_input[i] = (float)i / {input_size}.0f;
    }}
    
    {model_name}_forward(test_input, test_output);
    
    printf("C Inference Output:\\n");
    for (size_t i = 0; i < {output_size}; i++) {{
        printf("  [%zu] = %.6f\\n", i, test_output[i]);
    }}
}}
#endif
"""
    return source

# compiles model and returns compiled model object
def compile_model(
    model: onnx.ModelProto,
    model_info: dict,
    model_name: str = "model",
    target_chip: str = "STM32F401"
) -> CompiledModel:

    safe_name = model_name.replace("-", "_").replace(".", "_").replace(" ", "_").lower()
    
    header = generate_header(safe_name, model_info, target_chip)
    source = generate_source(safe_name, model_info, model, target_chip)
    
    return CompiledModel(
        source_code=source,
        header_code=header,
        model_name=safe_name
    )
